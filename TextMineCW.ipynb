{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InaJaweed/Text_Mining-CS3TM20/blob/main/TextMineCW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prior study (Codes in Week 5 folder）**:\n",
        "\n",
        "Please use this page as companion to understand the newsgroup data set.\n",
        "[Data Set](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
        "\n",
        "You will also need to be familiar with some text processing commands：\n",
        "\n",
        "[Tf-idf](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
        "\n",
        "[countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n"
      ],
      "metadata": {
        "id": "1ZVVMai8xRFX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhgYu18FM-_L"
      },
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXdrc29D71MH"
      },
      "source": [
        "# **Steps outline**\n",
        "1. Download your data set by inputting your student number.\n",
        "2. Process your text data, extract features, convert them into vectors\n",
        "3. Modeling, train models on the data set (select model, tune different parameters)\n",
        "4. Process your text data, extract features, convert them into vectors\n",
        "5. Analysis and discussions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "934uDa63sRnp"
      },
      "source": [
        "# Step 1: Load Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prior study (Codes in Week 5 folder）:\n",
        "\n",
        "Please use this page as companion to understand  [**Newsgroup data set**](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n"
      ],
      "metadata": {
        "id": "2k-V81t0z_NF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxDjTTcDrtVl"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train',  categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8VrHduYC-kd"
      },
      "source": [
        "**This is how to identify which data set to use (Please copy  the following information in report front   page).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvU6i2KNHzC4",
        "outputId": "476f6f1e-74fa-45a4-d18e-59884b37ac3f"
      },
      "source": [
        "index=input('type your student number?')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type your student number?30020691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=divmod(int(index),4)\n",
        "yourdata1=x[1]\n",
        "y=divmod(int(index),3)\n",
        "yourdata2=y[1]\n",
        "\n",
        "print('This is your data set index ----> (', x[1], y[1], ')' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8Gmf-HdGcWV",
        "outputId": "b31add30-7a93-46ab-e23c-942cbf95550a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is your data set index ----> ( 3 0 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXPpVRSGAPM7",
        "outputId": "d56c354d-cd9f-45ba-8491-3e83756db18c"
      },
      "source": [
        "data1= twenty_train.target_names[x[1]]\n",
        "data2= twenty_train.target_names[y[1]]\n",
        "categories1=[data1,data2]\n",
        "print(categories1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['soc.religion.christian', 'alt.atheism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNjBtO7-DOsu"
      },
      "source": [
        "**Your front page data information Ends here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1jAHpjtaSPu"
      },
      "source": [
        "# Step 2 Process your text data, extract features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El_vU9NocxVC"
      },
      "source": [
        "# 2.1 An example of preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDHwDyKzNirS"
      },
      "source": [
        "**An example is provided.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please pay attention  comment #replace ..., which means you need to change example text to your data set.\n",
        "Use google search for usages of  \"nltk tokenizer ”, \"nltk stemmer\", \"nltk pos tag\" to help your report writing."
      ],
      "metadata": {
        "id": "utJ6ap1bacK3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC3yT07PJnKp",
        "outputId": "fbdb5560-bf03-42a3-f966-0871962008c1"
      },
      "source": [
        "# write your own NLP precessing examples with  preprocessing techniques.\n",
        "\n",
        "# print(twenty_train.target_names[1])\n",
        "data1=twenty_train.data[1]\n",
        "print(data1)\n",
        "# please   replace 1 in bracket to other data sample and explore the code\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n",
            "Subject: help: Splitting a trimming region along a mesh \n",
            "Organization: University Of Kentucky, Dept. of Math Sciences\n",
            "Lines: 28\n",
            "\n",
            "\n",
            "\n",
            "\tHi,\n",
            "\n",
            "\tI have a problem, I hope some of the 'gurus' can help me solve.\n",
            "\n",
            "\tBackground of the problem:\n",
            "\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n",
            "\tmapping of a 3d Bezier patch into 2d. The area in this domain\n",
            "\twhich is inside a trimming loop had to be rendered. The trimming\n",
            "\tloop is a set of 2d Bezier curve segments.\n",
            "\tFor the sake of notation: the mesh is made up of cells.\n",
            "\n",
            "\tMy problem is this :\n",
            "\tThe trimming area has to be split up into individual smaller\n",
            "\tcells bounded by the trimming curve segments. If a cell\n",
            "\tis wholly inside the area...then it is output as a whole ,\n",
            "\telse it is trivially rejected. \n",
            "\n",
            "\tDoes any body know how thiss can be done, or is there any algo. \n",
            "\tsomewhere for doing this.\n",
            "\n",
            "\tAny help would be appreciated.\n",
            "\n",
            "\tThanks, \n",
            "\tAni.\n",
            "-- \n",
            "To get irritated is human, to stay cool, divine.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af4f3c8-d618-45ea-c0fd-d5df29bee9bf",
        "id": "cRHBOWMnAAWA"
      },
      "source": [
        "\n",
        "# tokenize: search: nltk tokenize\n",
        "example = \"This is an example sentence.\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "example_tokenize =word_tokenize(example)\n",
        "example_tokenize= word_tokenize(data1) # replace example in bracket to dataset.\n",
        "print(\"-------------------------tokenize:\")\n",
        "print(example_tokenize)\n",
        "print(\"Token Qty:\",len(example_tokenize))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------tokenize:\n",
            "['From', ':', 'ani', '@', 'ms.uky.edu', '(', 'Aniruddha', 'B.', 'Deglurkar', ')', 'Subject', ':', 'help', ':', 'Splitting', 'a', 'trimming', 'region', 'along', 'a', 'mesh', 'Organization', ':', 'University', 'Of', 'Kentucky', ',', 'Dept', '.', 'of', 'Math', 'Sciences', 'Lines', ':', '28', 'Hi', ',', 'I', 'have', 'a', 'problem', ',', 'I', 'hope', 'some', 'of', 'the', \"'gurus\", \"'\", 'can', 'help', 'me', 'solve', '.', 'Background', 'of', 'the', 'problem', ':', 'I', 'have', 'a', 'rectangular', 'mesh', 'in', 'the', 'uv', 'domain', ',', 'i.e', 'the', 'mesh', 'is', 'a', 'mapping', 'of', 'a', '3d', 'Bezier', 'patch', 'into', '2d', '.', 'The', 'area', 'in', 'this', 'domain', 'which', 'is', 'inside', 'a', 'trimming', 'loop', 'had', 'to', 'be', 'rendered', '.', 'The', 'trimming', 'loop', 'is', 'a', 'set', 'of', '2d', 'Bezier', 'curve', 'segments', '.', 'For', 'the', 'sake', 'of', 'notation', ':', 'the', 'mesh', 'is', 'made', 'up', 'of', 'cells', '.', 'My', 'problem', 'is', 'this', ':', 'The', 'trimming', 'area', 'has', 'to', 'be', 'split', 'up', 'into', 'individual', 'smaller', 'cells', 'bounded', 'by', 'the', 'trimming', 'curve', 'segments', '.', 'If', 'a', 'cell', 'is', 'wholly', 'inside', 'the', 'area', '...', 'then', 'it', 'is', 'output', 'as', 'a', 'whole', ',', 'else', 'it', 'is', 'trivially', 'rejected', '.', 'Does', 'any', 'body', 'know', 'how', 'thiss', 'can', 'be', 'done', ',', 'or', 'is', 'there', 'any', 'algo', '.', 'somewhere', 'for', 'doing', 'this', '.', 'Any', 'help', 'would', 'be', 'appreciated', '.', 'Thanks', ',', 'Ani', '.', '--', 'To', 'get', 'irritated', 'is', 'human', ',', 'to', 'stay', 'cool', ',', 'divine', '.']\n",
            "Token Qty: 216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4839122e-3b82-494a-8fe8-0f4ab24a2785",
        "id": "kAaaP86_Ahmo"
      },
      "source": [
        "# stemmer: search: nltk stemmer\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "example_stem = stemmer.stem(data1)  # replace .....\n",
        "print(\"-------------------------stem:\")\n",
        "print(example_stem)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------stem:\n",
            "from: ani@ms.uky.edu (aniruddha b. deglurkar)\n",
            "subject: help: splitting a trimming region along a mesh \n",
            "organization: university of kentucky, dept. of math sciences\n",
            "lines: 28\n",
            "\n",
            "\n",
            "\n",
            "\thi,\n",
            "\n",
            "\ti have a problem, i hope some of the 'gurus' can help me solve.\n",
            "\n",
            "\tbackground of the problem:\n",
            "\ti have a rectangular mesh in the uv domain, i.e  the mesh is a \n",
            "\tmapping of a 3d bezier patch into 2d. the area in this domain\n",
            "\twhich is inside a trimming loop had to be rendered. the trimming\n",
            "\tloop is a set of 2d bezier curve segments.\n",
            "\tfor the sake of notation: the mesh is made up of cells.\n",
            "\n",
            "\tmy problem is this :\n",
            "\tthe trimming area has to be split up into individual smaller\n",
            "\tcells bounded by the trimming curve segments. if a cell\n",
            "\tis wholly inside the area...then it is output as a whole ,\n",
            "\telse it is trivially rejected. \n",
            "\n",
            "\tdoes any body know how thiss can be done, or is there any algo. \n",
            "\tsomewhere for doing this.\n",
            "\n",
            "\tany help would be appreciated.\n",
            "\n",
            "\tthanks, \n",
            "\tani.\n",
            "-- \n",
            "to get irritated is human, to stay cool, divine.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_taging: search: nltk pos tagging example\n",
        "example_posTag=nltk.pos_tag(example_tokenize)\n",
        "print(\"-------------------------pos_taging:\")\n",
        "print(example_posTag)"
      ],
      "metadata": {
        "id": "iL4Vr-m0ApLu",
        "outputId": "3adb73f0-37d8-4d1f-cb1e-cc2b34491756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------pos_taging:\n",
            "[('From', 'IN'), (':', ':'), ('ani', 'NN'), ('@', 'NN'), ('ms.uky.edu', 'NN'), ('(', '('), ('Aniruddha', 'NNP'), ('B.', 'NNP'), ('Deglurkar', 'NNP'), (')', ')'), ('Subject', 'NN'), (':', ':'), ('help', 'NN'), (':', ':'), ('Splitting', 'VBG'), ('a', 'DT'), ('trimming', 'JJ'), ('region', 'NN'), ('along', 'IN'), ('a', 'DT'), ('mesh', 'JJ'), ('Organization', 'NN'), (':', ':'), ('University', 'NNP'), ('Of', 'IN'), ('Kentucky', 'NNP'), (',', ','), ('Dept', 'NNP'), ('.', '.'), ('of', 'IN'), ('Math', 'NNP'), ('Sciences', 'NNPS'), ('Lines', 'NNPS'), (':', ':'), ('28', 'CD'), ('Hi', 'NNP'), (',', ','), ('I', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('problem', 'NN'), (',', ','), ('I', 'PRP'), ('hope', 'VBP'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), (\"'gurus\", 'NNP'), (\"'\", 'POS'), ('can', 'MD'), ('help', 'VB'), ('me', 'PRP'), ('solve', 'VB'), ('.', '.'), ('Background', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('problem', 'NN'), (':', ':'), ('I', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('rectangular', 'JJ'), ('mesh', 'NN'), ('in', 'IN'), ('the', 'DT'), ('uv', 'JJ'), ('domain', 'NN'), (',', ','), ('i.e', 'VBP'), ('the', 'DT'), ('mesh', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('mapping', 'NN'), ('of', 'IN'), ('a', 'DT'), ('3d', 'CD'), ('Bezier', 'NNP'), ('patch', 'NN'), ('into', 'IN'), ('2d', 'CD'), ('.', '.'), ('The', 'DT'), ('area', 'NN'), ('in', 'IN'), ('this', 'DT'), ('domain', 'NN'), ('which', 'WDT'), ('is', 'VBZ'), ('inside', 'IN'), ('a', 'DT'), ('trimming', 'NN'), ('loop', 'NN'), ('had', 'VBD'), ('to', 'TO'), ('be', 'VB'), ('rendered', 'VBN'), ('.', '.'), ('The', 'DT'), ('trimming', 'VBG'), ('loop', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('set', 'NN'), ('of', 'IN'), ('2d', 'CD'), ('Bezier', 'NNP'), ('curve', 'NN'), ('segments', 'NNS'), ('.', '.'), ('For', 'IN'), ('the', 'DT'), ('sake', 'NN'), ('of', 'IN'), ('notation', 'NN'), (':', ':'), ('the', 'DT'), ('mesh', 'NN'), ('is', 'VBZ'), ('made', 'VBN'), ('up', 'IN'), ('of', 'IN'), ('cells', 'NNS'), ('.', '.'), ('My', 'PRP$'), ('problem', 'NN'), ('is', 'VBZ'), ('this', 'DT'), (':', ':'), ('The', 'DT'), ('trimming', 'VBG'), ('area', 'NN'), ('has', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('split', 'VBN'), ('up', 'RP'), ('into', 'IN'), ('individual', 'JJ'), ('smaller', 'JJR'), ('cells', 'NNS'), ('bounded', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('trimming', 'VBG'), ('curve', 'NN'), ('segments', 'NNS'), ('.', '.'), ('If', 'IN'), ('a', 'DT'), ('cell', 'NN'), ('is', 'VBZ'), ('wholly', 'RB'), ('inside', 'IN'), ('the', 'DT'), ('area', 'NN'), ('...', ':'), ('then', 'RB'), ('it', 'PRP'), ('is', 'VBZ'), ('output', 'NN'), ('as', 'IN'), ('a', 'DT'), ('whole', 'NN'), (',', ','), ('else', 'VB'), ('it', 'PRP'), ('is', 'VBZ'), ('trivially', 'RB'), ('rejected', 'VBN'), ('.', '.'), ('Does', 'VBZ'), ('any', 'DT'), ('body', 'NN'), ('know', 'VB'), ('how', 'WRB'), ('thiss', 'JJ'), ('can', 'MD'), ('be', 'VB'), ('done', 'VBN'), (',', ','), ('or', 'CC'), ('is', 'VBZ'), ('there', 'RB'), ('any', 'DT'), ('algo', 'NN'), ('.', '.'), ('somewhere', 'RB'), ('for', 'IN'), ('doing', 'VBG'), ('this', 'DT'), ('.', '.'), ('Any', 'DT'), ('help', 'NN'), ('would', 'MD'), ('be', 'VB'), ('appreciated', 'VBN'), ('.', '.'), ('Thanks', 'NNS'), (',', ','), ('Ani', 'NNP'), ('.', '.'), ('--', ':'), ('To', 'TO'), ('get', 'VB'), ('irritated', 'JJ'), ('is', 'VBZ'), ('human', 'JJ'), (',', ','), ('to', 'TO'), ('stay', 'VB'), ('cool', 'JJ'), (',', ','), ('divine', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec39ff3-57c9-4387-f6bc-2d0311c99e56",
        "id": "O-n6AAbc_-_5"
      },
      "source": [
        " # consituency parsing, chunking\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(example_posTag)\n",
        "print(result)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  From/IN\n",
            "  :/:\n",
            "  (NP ani/NN)\n",
            "  (NP @/NN)\n",
            "  (NP ms.uky.edu/NN)\n",
            "  (/(\n",
            "  Aniruddha/NNP\n",
            "  B./NNP\n",
            "  Deglurkar/NNP\n",
            "  )/)\n",
            "  (NP Subject/NN)\n",
            "  :/:\n",
            "  (NP help/NN)\n",
            "  :/:\n",
            "  Splitting/VBG\n",
            "  (NP a/DT trimming/JJ region/NN)\n",
            "  along/IN\n",
            "  (NP a/DT mesh/JJ Organization/NN)\n",
            "  :/:\n",
            "  University/NNP\n",
            "  Of/IN\n",
            "  Kentucky/NNP\n",
            "  ,/,\n",
            "  Dept/NNP\n",
            "  ./.\n",
            "  of/IN\n",
            "  Math/NNP\n",
            "  Sciences/NNPS\n",
            "  Lines/NNPS\n",
            "  :/:\n",
            "  28/CD\n",
            "  Hi/NNP\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  have/VBP\n",
            "  (NP a/DT problem/NN)\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  hope/VBP\n",
            "  some/DT\n",
            "  of/IN\n",
            "  the/DT\n",
            "  'gurus/NNP\n",
            "  '/POS\n",
            "  can/MD\n",
            "  help/VB\n",
            "  me/PRP\n",
            "  solve/VB\n",
            "  ./.\n",
            "  Background/NNP\n",
            "  of/IN\n",
            "  (NP the/DT problem/NN)\n",
            "  :/:\n",
            "  I/PRP\n",
            "  have/VBP\n",
            "  (NP a/DT rectangular/JJ mesh/NN)\n",
            "  in/IN\n",
            "  (NP the/DT uv/JJ domain/NN)\n",
            "  ,/,\n",
            "  i.e/VBP\n",
            "  (NP the/DT mesh/NN)\n",
            "  is/VBZ\n",
            "  (NP a/DT mapping/NN)\n",
            "  of/IN\n",
            "  a/DT\n",
            "  3d/CD\n",
            "  Bezier/NNP\n",
            "  (NP patch/NN)\n",
            "  into/IN\n",
            "  2d/CD\n",
            "  ./.\n",
            "  (NP The/DT area/NN)\n",
            "  in/IN\n",
            "  (NP this/DT domain/NN)\n",
            "  which/WDT\n",
            "  is/VBZ\n",
            "  inside/IN\n",
            "  (NP a/DT trimming/NN)\n",
            "  (NP loop/NN)\n",
            "  had/VBD\n",
            "  to/TO\n",
            "  be/VB\n",
            "  rendered/VBN\n",
            "  ./.\n",
            "  The/DT\n",
            "  trimming/VBG\n",
            "  (NP loop/NN)\n",
            "  is/VBZ\n",
            "  (NP a/DT set/NN)\n",
            "  of/IN\n",
            "  2d/CD\n",
            "  Bezier/NNP\n",
            "  (NP curve/NN)\n",
            "  segments/NNS\n",
            "  ./.\n",
            "  For/IN\n",
            "  (NP the/DT sake/NN)\n",
            "  of/IN\n",
            "  (NP notation/NN)\n",
            "  :/:\n",
            "  (NP the/DT mesh/NN)\n",
            "  is/VBZ\n",
            "  made/VBN\n",
            "  up/IN\n",
            "  of/IN\n",
            "  cells/NNS\n",
            "  ./.\n",
            "  My/PRP$\n",
            "  (NP problem/NN)\n",
            "  is/VBZ\n",
            "  this/DT\n",
            "  :/:\n",
            "  The/DT\n",
            "  trimming/VBG\n",
            "  (NP area/NN)\n",
            "  has/VBZ\n",
            "  to/TO\n",
            "  be/VB\n",
            "  split/VBN\n",
            "  up/RP\n",
            "  into/IN\n",
            "  individual/JJ\n",
            "  smaller/JJR\n",
            "  cells/NNS\n",
            "  bounded/VBN\n",
            "  by/IN\n",
            "  the/DT\n",
            "  trimming/VBG\n",
            "  (NP curve/NN)\n",
            "  segments/NNS\n",
            "  ./.\n",
            "  If/IN\n",
            "  (NP a/DT cell/NN)\n",
            "  is/VBZ\n",
            "  wholly/RB\n",
            "  inside/IN\n",
            "  (NP the/DT area/NN)\n",
            "  .../:\n",
            "  then/RB\n",
            "  it/PRP\n",
            "  is/VBZ\n",
            "  (NP output/NN)\n",
            "  as/IN\n",
            "  (NP a/DT whole/NN)\n",
            "  ,/,\n",
            "  else/VB\n",
            "  it/PRP\n",
            "  is/VBZ\n",
            "  trivially/RB\n",
            "  rejected/VBN\n",
            "  ./.\n",
            "  Does/VBZ\n",
            "  (NP any/DT body/NN)\n",
            "  know/VB\n",
            "  how/WRB\n",
            "  thiss/JJ\n",
            "  can/MD\n",
            "  be/VB\n",
            "  done/VBN\n",
            "  ,/,\n",
            "  or/CC\n",
            "  is/VBZ\n",
            "  there/RB\n",
            "  (NP any/DT algo/NN)\n",
            "  ./.\n",
            "  somewhere/RB\n",
            "  for/IN\n",
            "  doing/VBG\n",
            "  this/DT\n",
            "  ./.\n",
            "  (NP Any/DT help/NN)\n",
            "  would/MD\n",
            "  be/VB\n",
            "  appreciated/VBN\n",
            "  ./.\n",
            "  Thanks/NNS\n",
            "  ,/,\n",
            "  Ani/NNP\n",
            "  ./.\n",
            "  --/:\n",
            "  To/TO\n",
            "  get/VB\n",
            "  irritated/JJ\n",
            "  is/VBZ\n",
            "  human/JJ\n",
            "  ,/,\n",
            "  to/TO\n",
            "  stay/VB\n",
            "  cool/JJ\n",
            "  ,/,\n",
            "  (NP divine/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7VpnVNpKuUt"
      },
      "source": [
        "#2.2 NLP Preprocesssing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmXyhqwOaUQ"
      },
      "source": [
        "**Some preprocessing are provided for convenience. Please include why NLP preprocessing is in your report. Explain what techniques have been experimented in your report.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8u5y9adK3tc",
        "outputId": "e0601add-2855-4d76-964b-cb4b74127012"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "stopwordEn = stopwords.words('english')\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "def lemmaWord(word):\n",
        "    lemma = wordnet.morphy(word)\n",
        "    if lemma is not None:\n",
        "        return lemma\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "def stemWord(word):\n",
        "    stem = stemmer.stem(word)\n",
        "    if stem is not None:\n",
        "        return stem\n",
        "    else:\n",
        "        return word\n",
        "\n",
        "def processText(text,lemma=False, gram=1, rmStop=True): # default remove stop words\n",
        "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE) #delete URL, #hashtag# , and @xxx\n",
        "    tokens = word_tokenize(text)\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    new_tokens = []\n",
        "    stoplist = stopwordEn if rmStop else []\n",
        "    for i in tokens:\n",
        "      i = i.lower()\n",
        "      if i.isalpha() and (i not in stoplist or i in whitelist):  #i not in ['.',',',';']  and (...)\n",
        "        if lemma: i = lemmaWord(i)\n",
        "        new_tokens.append(i)\n",
        "    del tokens\n",
        "    # tokens = [lemmaWord(i.lower()) if lemma else i.lower() for i in tokens if (i.lower() not in stoplist or i.lower() in whitelist) and i.isalpha()]\n",
        "    if gram<=1:\n",
        "        return new_tokens\n",
        "    else:\n",
        "        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKFoZaWSVrnq"
      },
      "source": [
        "def getTags(text):\n",
        "  token = word_tokenize(text)\n",
        "  token = [l.lower() for l in token]\n",
        "  train_tags = nltk.pos_tag(token)\n",
        "  return [i[1] for i in train_tags]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8mwYOcFcS02",
        "outputId": "ae73dc78-877c-4d7f-b7bd-2185f9a54da8"
      },
      "source": [
        "print(processText(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aniruddha', 'deglurkar', 'subject', 'help', 'splitting', 'trimming', 'region', 'along', 'mesh', 'organization', 'university', 'kentucky', 'dept', 'math', 'sciences', 'lines', 'hi', 'problem', 'hope', 'help', 'solve', 'background', 'problem', 'rectangular', 'mesh', 'uv', 'domain', 'mesh', 'mapping', 'bezier', 'patch', 'area', 'domain', 'inside', 'trimming', 'loop', 'rendered', 'trimming', 'loop', 'set', 'bezier', 'curve', 'segments', 'sake', 'notation', 'mesh', 'made', 'cells', 'problem', 'trimming', 'area', 'split', 'individual', 'smaller', 'cells', 'bounded', 'trimming', 'curve', 'segments', 'cell', 'wholly', 'inside', 'area', 'output', 'whole', 'else', 'trivially', 'rejected', 'body', 'know', 'thiss', 'done', 'algo', 'somewhere', 'help', 'would', 'appreciated', 'thanks', 'ani', 'get', 'irritated', 'human', 'stay', 'cool', 'divine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZRqySa8cbr1",
        "outputId": "2faa143a-a9de-4a40-a30c-3618fc322a93"
      },
      "source": [
        "print(getTags(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['IN', ':', 'NN', 'NN', 'NN', '(', 'JJ', 'NN', 'NN', ')', 'NN', ':', 'NN', ':', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'IN', 'NN', ',', 'NN', '.', 'IN', 'NN', 'NNS', 'NNS', ':', 'CD', 'NN', ',', 'NN', 'VBP', 'DT', 'NN', ',', 'NN', 'VBP', 'DT', 'IN', 'DT', 'NNP', 'POS', 'MD', 'VB', 'PRP', 'VB', '.', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'VBP', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'CD', 'NN', 'NN', 'IN', 'CD', '.', 'DT', 'NN', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'NN', 'VBD', 'TO', 'VB', 'VBN', '.', 'DT', 'VBG', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'NN', 'NNS', '.', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'IN', 'NNS', '.', 'PRP$', 'NN', 'VBZ', 'DT', ':', 'DT', 'VBG', 'NN', 'VBZ', 'TO', 'VB', 'VBN', 'RP', 'IN', 'JJ', 'JJR', 'NNS', 'VBN', 'IN', 'DT', 'VBG', 'NN', 'NNS', '.', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'IN', 'DT', 'NN', ':', 'RB', 'PRP', 'VBZ', 'NN', 'IN', 'DT', 'NN', ',', 'VB', 'PRP', 'VBZ', 'RB', 'VBN', '.', 'VBZ', 'DT', 'NN', 'VB', 'WRB', 'JJ', 'MD', 'VB', 'VBN', ',', 'CC', 'VBZ', 'RB', 'DT', 'NN', '.', 'RB', 'IN', 'VBG', 'DT', '.', 'DT', 'NN', 'MD', 'VB', 'VBN', '.', 'NNS', ',', 'NN', '.', ':', 'TO', 'VB', 'JJ', 'VBZ', 'JJ', ',', 'TO', 'VB', 'JJ', ',', 'NN', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44xTvpLa_UC9"
      },
      "source": [
        "# Step 3: Build a Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g5g93owSogu"
      },
      "source": [
        "**Modify the block code below to your choice of classifier [link text](https://www.nltk.org/book/ch06.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGs1A8S1TMTi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9HMKvgGMHPB",
        "outputId": "c24cd633-6d0b-4255-bbc3-c67577adf613"
      },
      "source": [
        "print(categories1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['soc.religion.christian', 'comp.graphics']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without modification, the code will output all four classes.\n",
        "\n",
        "\n",
        "I included some commented codes in places where you may use to change to two class data sets   from your student number, and use logistic model.\n",
        "Your data sets can be obtained as twenty_train1, twenty_test1. All  data set names can be adjusted to get it right."
      ],
      "metadata": {
        "id": "Yl4relrvUjHt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PDFkEEiL1GQ"
      },
      "source": [
        "#twenty_train1 = fetch_20newsgroups(subset='train',  categories=categories1, shuffle=True, random_state=42)\n",
        "#twenty_test1 = fetch_20newsgroups(subset='test',  categories=categories1, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNm3axlhdzlF"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "\n",
        "# Level: lexicon, model: tf-idf\n",
        "text_clf = Pipeline([\n",
        "    # add your code about text processing\n",
        "    ('vect', CountVectorizer(analyzer=processText)),\n",
        "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "\n",
        "    # change your classifier here, search: sklearn logistic regression example\n",
        "    ('clf', SGDClassifier())\n",
        "   #  ('clf', LogisticRegression())\n",
        "\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vuq37Bf3Qjpn",
        "outputId": "07301f41-58ea-43b1-cdd1-9865e3888004"
      },
      "source": [
        "# To train the model\n",
        "text_clf.fit(twenty_train.data, twenty_train.target)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect',\n",
              "                 CountVectorizer(analyzer=<function processText at 0x7fa688dfd8c0>)),\n",
              "                ('tfidf', TfidfTransformer()), ('clf', SGDClassifier())])"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjQ8DmPNRUuJ"
      },
      "source": [
        "# Step 4: Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMdoIHjMRWce"
      },
      "source": [
        "# To make prediction with dev/test set\n",
        "predicted = text_clf.predict(twenty_test.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GXHJHqoBmyJ"
      },
      "source": [
        "# Step 5: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You need to modify the code so only two classes from your student number are output as matrix.**"
      ],
      "metadata": {
        "id": "QuBuHl6G5xhN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "LdB9js0QDErf",
        "outputId": "a73b878c-14db-4078-a7e6-cb4a0ca9d259"
      },
      "source": [
        "# To evaluate your prediction on dev set\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy:\", metrics.accuracy_score(twenty_test.target, predicted))\n",
        "\n",
        "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n",
        "\n",
        "# confusion class\n",
        "pd.DataFrame(metrics.confusion_matrix(twenty_test.target, predicted),\n",
        "             columns=twenty_test.target_names,index=twenty_test.target_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9161118508655126\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "           alt.atheism       0.91      0.80      0.85       319\n",
            "         comp.graphics       0.94      0.96      0.95       389\n",
            "               sci.med       0.94      0.94      0.94       396\n",
            "soc.religion.christian       0.88      0.94      0.91       398\n",
            "\n",
            "              accuracy                           0.92      1502\n",
            "             macro avg       0.92      0.91      0.91      1502\n",
            "          weighted avg       0.92      0.92      0.92      1502\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e9ddb6ed-48f5-4bb1-81f7-60b3c9ff11fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alt.atheism</th>\n",
              "      <th>comp.graphics</th>\n",
              "      <th>sci.med</th>\n",
              "      <th>soc.religion.christian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>alt.atheism</th>\n",
              "      <td>256</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comp.graphics</th>\n",
              "      <td>6</td>\n",
              "      <td>373</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sci.med</th>\n",
              "      <td>7</td>\n",
              "      <td>12</td>\n",
              "      <td>372</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>soc.religion.christian</th>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9ddb6ed-48f5-4bb1-81f7-60b3c9ff11fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e9ddb6ed-48f5-4bb1-81f7-60b3c9ff11fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e9ddb6ed-48f5-4bb1-81f7-60b3c9ff11fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                        alt.atheism  ...  soc.religion.christian\n",
              "alt.atheism                     256  ...                      46\n",
              "comp.graphics                     6  ...                       1\n",
              "sci.med                           7  ...                       5\n",
              "soc.religion.christian           11  ...                     375\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCLCqFXPQsRq"
      },
      "source": [
        "# Step 6: Error Analysis and Discussion\n",
        "write down your own obseration about the predictions. Consider both confusion matrix and selected examples. Which classes are predicted correctly or incorrecly, possible explaination, possible solutions\n",
        "\n",
        "Exmaple: 1) Lab Practical, which feature is helpful for female name classification. https://www.nltk.org/book/ch06.html\n",
        "2) research paper: https://github.com/yoonkim/CNN_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "kvBw9qkKDS-m",
        "outputId": "91a2c65d-8a64-46be-f5f8-640277fa2a57"
      },
      "source": [
        "df_pred = pd.DataFrame({'news':twenty_test.data,'prediction':predicted, 'true':twenty_test.target})\n",
        "df_pred[df_pred['true'] != df_pred['prediction']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b1605e42-9a31-4009-b66a-197dfd6d8c53\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news</th>\n",
              "      <th>prediction</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>From: \"Gabriel D. Underwood\" &lt;gabe+@CMU.EDU&gt;\\n...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>From: swf@elsegundoca.ncr.com (Stan Friesen)\\n...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: D...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>From: UC512052@mizzou1.missouri.edu (David K. ...</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Organization: Penn State University\\nFrom: &lt;RF...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>From: alan.barclay@almac.co.uk (Alan Barclay)\\...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>From: pww@spacsun.rice.edu (Peter Walker)\\nSub...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1480</th>\n",
              "      <td>From: wilsonr@logica.co.uk\\nSubject: Re: What ...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492</th>\n",
              "      <td>From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>126 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1605e42-9a31-4009-b66a-197dfd6d8c53')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b1605e42-9a31-4009-b66a-197dfd6d8c53 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b1605e42-9a31-4009-b66a-197dfd6d8c53');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   news  prediction  true\n",
              "12    From: \"Gabriel D. Underwood\" <gabe+@CMU.EDU>\\n...           3     2\n",
              "15    From: swf@elsegundoca.ncr.com (Stan Friesen)\\n...           2     0\n",
              "19    From: mathew <mathew@mantis.co.uk>\\nSubject: D...           3     0\n",
              "26    From: UC512052@mizzou1.missouri.edu (David K. ...           2     1\n",
              "36    Organization: Penn State University\\nFrom: <RF...           0     2\n",
              "...                                                 ...         ...   ...\n",
              "1438  From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...           0     3\n",
              "1450  From: alan.barclay@almac.co.uk (Alan Barclay)\\...           1     2\n",
              "1455  From: pww@spacsun.rice.edu (Peter Walker)\\nSub...           3     0\n",
              "1480  From: wilsonr@logica.co.uk\\nSubject: Re: What ...           3     0\n",
              "1492  From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...           3     0\n",
              "\n",
              "[126 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRIRg2rufjPu"
      },
      "source": [
        "#References:  \n",
        "\n",
        "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "\n",
        "https://www.nltk.org/book/ch06.html\n",
        "\n",
        "search: Other online resources:\n",
        "\n",
        "https://towardsdatascience.com/setting-up-text-preprocessing-pipeline-using-scikit-learn-and-spacy-e09b9b76758f\n",
        "\n",
        "sentiment analysis scikit learn\n",
        "\n",
        "scikit learn or nltk + NLP techniques\n",
        "\n",
        "python + NLP techniques\n",
        "\n",
        "scikit learn logistic regression\n",
        "\n",
        "\n"
      ]
    }
  ]
}